# -*- coding: utf-8 -*-
"""CnnAl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DX8dB3f4-0ul4Gb2VcAq6LVzkwzggN6b
"""

import keras as k
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import time
from keras.applications.vgg16 import VGG16
# Some utils
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing import  image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import load_img
from tensorflow.keras.utils import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.applications.vgg16 import VGG16
import matplotlib.pyplot as plt
import numpy as np
import os

myTrainDir =os.path(r"C:\Users\agusa\OneDrive\Escritorio\Ing\IA\DataResos\DatasetSanoONo")
target_sz = (128, 128)


train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,validation_split=0.2)

train_generator = train_datagen.flow_from_directory(myTrainDir,
                                                 target_size=target_sz,
                                                 batch_size=20,
                                                 class_mode='categorical',
                                                 shuffle=False,
                                                 seed=20,
                                                 subset="training" )

validation_generator = train_datagen.flow_from_directory(myTrainDir,
                                                 target_size=target_sz,
                                                 batch_size=20,
                                                 class_mode='categorical',
                                                 shuffle=False,
                                                 seed=20,
                                                 subset="validation")

xTrain=np.concatenate([train_generator.next()[0] for i in range(train_generator.__len__())])
yTrain=np.concatenate([train_generator.next()[1] for i in range(train_generator.__len__())])
print(xTrain.shape)
print(yTrain.shape)

xTest=np.concatenate([validation_generator.next()[0] for i in range(validation_generator.__len__())])
yTest=np.concatenate([validation_generator.next()[1] for i in range(validation_generator.__len__())])
print(xTest.shape)
print(yTest.shape)

labels = list(train_generator.class_indices.keys())
print(labels)

shuffler = np. random. permutation(len(yTrain))
xTrain = xTrain[shuffler]
yTrain = yTrain[shuffler]

shuffler = np. random. permutation(len(yTest))
xTest = xTest[shuffler]
yTest = yTest[shuffler]

# load model
model = VGG16()
# summarize the model
model.summary()

n=6

plt.imshow(x1[n])
print(y1[n])
print(labels[np.argmax(y1[n])])

lb = [labels[np.argmax(x)] for x in y1[:10]]
for lb_enc, lb in zip(y1[:10],lb):
    print(lb_enc,"-->",lb)

from tensorflow.keras.models import Model
from tensorflow.keras.models import  Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout, MaxPooling2D, GlobalAveragePooling2D
from tensorflow.keras.optimizers import  RMSprop, Adam, Adamax

num_class = 4  # Cantidad de clases a clasificar


base_model = VGG16(include_top=False, weights='imagenet', input_shape=(128,128,3))

# COMIENZO A AGREGAR DISTINTAS CAPAS
# NOTA: ESTO TAMBIEN SE PUEDE HACER MEDIATE UNA RED SECUENCIAL
x = base_model.output
# AGREGAMOS ALGUNAS CAPAS A LA RED BASE
x = GlobalAveragePooling2D(name="GAP2D_Al")(x)
x = Dense(512,activation='relu', name="Dense1_Al")(x)
x = Dropout(0.5, name="Drop1_Al")(x)
x = Dense(512,activation='tanh', name="Dense2_Al")(x)
x = Dropout(0.5, name="Drop2_Al")(x)
# FINALMENTE AGREGO UNA CAPA SOFTMAX CON 4 CLASES
preds = Dense(num_class, activation='softmax', name="Softmax1_Al")(x)

# ARMO EL MODELO COMPLETO
model = Model(inputs=base_model.input, outputs=preds)


# Freeze four convolution blocks
for layer in model.layers[:19]:
    layer.trainable = False
# Make sure you have frozen the correct layers
for i, layer in enumerate(model.layers):
    print(i, layer.name, layer.trainable)

model.summary()

# DEFINO LA CANTIDAD DE EPOCAS DE ENTRENAMIENTO
# ATENCION! N0 HAGO EARLY STOP
epochs = 30
# DEFINO LA TASA DE APRENDIZAJE
learning_rate = 0.001
# 00 - lr = 0.0005
# 01 - lr = 0.001

# DEFINO LA TASA DE CAIDA
# decay_rate = learning_rate / epochs
#decay_rate = 0.01
# 00 - decay = learning_rate / epochs
# 01 - decay = 0.01
# 02 - decay = 0.001

# DEFINO EL OPTIMIZADOR
# opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=decay_rate, amsgrad=False)
opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=0.1, amsgrad=False)
#opt2 = Adamax( learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=0.02)
# COMPILO EL MODELO - USO CATEGORICAL CROSSENTROPY COMO LOSS FUNCTION
model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['categorical_accuracy'])

history = model.fit(x1, y1, batch_size=150,epochs=5)


acc = history.history['categorical_accuracy']
loss = history.history['loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training categorical_accuracy')
plt.title('Training accuracy')

plt.figure()
plt.plot(epochs, loss, 'b', label='Training loss')
plt.title('Trainingloss')
plt.legend()

plt.show()